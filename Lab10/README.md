# Lab10
In this lab we must train a RL agent to play tic tac toe.
I employed Q-learning, here are they key points of my strategy:
- **State formulation**: the state is an array of ternary bits since each box can be ticked by two players or it can be not taken yet. With there are at most $3^9=19683$ states, even though there are much less because tic tac toe is turn based and the number of boxes occupied by the first player can only be equal or larger than one with respect to the number of boxes occupied by the second player. Additionally, there is a symmetry as that player 0 is equivalent to player 1, hence we can cut in half the number of possible states. In practice, the cardinality of the state space is lower than $10^4$.
- **Reward**: the reward system is quite naive as I did not want to hard-code complex strategies. It penalises moves that let the opponent win and rewards winning moves or moves that do not let the opponent win. Additionally, the algorithm rewards moves that occupy the largest number of straight lines.
- **Hyperparameters**: other than the usual parameters for Q-learning, I introduced a parameter $p$ for selecting a random move during training. The algorithm performs better with high learning rate (>0.1) and high $p$. It is not greatly impacted y the discount factor.
- **Tests**: I test the agent against a random agent, results show that convergence is relatively fast and that the trained agent cannot acomplish a perfect strategy but obtains good results.     
